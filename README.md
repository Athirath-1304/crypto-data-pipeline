# Crypto Data Pipeline ðŸš€

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Airflow](https://img.shields.io/badge/Airflow-Orchestrated-green)
![License](https://img.shields.io/badge/License-MIT-green)
![Last Commit](https://img.shields.io/github/last-commit/Athirath-1304/crypto-data-pipeline)
![CI](https://github.com/Athirath-1304/crypto-data-pipeline/actions/workflows/python-ci.yml/badge.svg)

---

A production-ready data engineering pipeline that ingests live cryptocurrency data from the [CoinGecko API](https://www.coingecko.com/), validates its schema, saves it as Parquet files, and uploads it to an AWS S3 data lake. The entire workflow is orchestrated using Apache Airflow and modularized for maintainability. A Streamlit dashboard provides a visual view of the crypto data with filters and time-based trends.

---

## ðŸ“Œ Key Features

- â›½ **Ingestion:** Live data from CoinGecko API (top 100 coins)  
- âœ… **Schema Validation:** Ensures structured, consistent data  
- ðŸ“€ **Storage:** Saves data locally as `.parquet` files  
- â˜ï¸ **Cloud Upload:** Automatically uploads to AWS S3  
- âš™ï¸ **Orchestration:** Apache Airflow DAG handles full flow  
- ðŸ”€ **Modular Design:** Split into extract, validate, load stages  
- ðŸ“Š **Streamlit Dashboard:** Visual insights with filters + trends  
- ðŸ” **Configurable:** Use `.env` for keys and bucket names  

---

## ðŸ› ï¸ Tech Stack

- **Python 3.10+**  
- **Apache Airflow** for orchestration  
- `pandas`, `requests`, `boto3`, `pyarrow`, `python-dotenv`  
- **AWS S3** for cloud storage  
- **Streamlit** for visualization  

---

## ðŸ§± Architecture

This project follows a modular pattern and uses Airflow for daily orchestration:

![Crypto Data Pipeline Architecture](assets/pipeline_architecture.png)

---

## ðŸ”§ Local Setup & Demo

```bash
# ðŸ“¦ Install dependencies
pip install -r requirements.txt

# ðŸ” Set up environment variables
# Create a .env file in the root directory with the following content:
echo "AWS_ACCESS_KEY_ID=your_access_key" >> .env
echo "AWS_SECRET_ACCESS_KEY=your_secret_key" >> .env
echo "S3_BUCKET_NAME=athirath-crypto-data-lake" >> .env

# ðŸ§ª Quick Local Demo (No AWS Required)

# Step 1: Disable S3 Upload
# In crypto_data_pipeline/dags/crypto_pipeline_dag.py, comment out the upload_to_s3 line:
# upload_to_s3(df, s3_bucket, filename)

# Step 2: Start Airflow
airflow standalone

# Step 3: Trigger the DAG manually
# Open Airflow UI at http://localhost:8080
# Enable and trigger the DAG: crypto_pipeline_dag

# Step 4: Check generated .parquet files
ls crypto_data_pipeline/data/

# ðŸ“Š Streamlit Dashboard

# Run the dashboard
streamlit run app.py

# Access dashboard at:
# http://localhost:8501
# Use the sidebar to:
# - View Top Gainers
# - View Top Losers
# - Filter by coin
# - View price trends and market caps
```

---

## ðŸ“‚ Folder Structure

```bash
crypto-data-pipeline/
â”œâ”€â”€ crypto_data_pipeline/
â”‚   â”œâ”€â”€ dags/                 # Airflow DAGs
â”‚   â”œâ”€â”€ data/                 # Local Parquet data files
â”‚   â”œâ”€â”€ extract/              # API ingestion logic
â”‚   â”œâ”€â”€ load/                 # AWS S3 upload logic
â”‚   â”œâ”€â”€ schema/               # Schema validation
â”‚   â””â”€â”€ ...
â”œâ”€â”€ app.py                   # Streamlit dashboard
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ README.md
â””â”€â”€ ...
```

---

## ðŸ“Œ Status

ðŸŸ¢ **Stable** â€” tested with local DAGs and dashboard.

---

## âœ… Next Goals

- [ ] Add unit tests  
- [ ] Integrate AWS Athena querying  
- [ ] Add GitHub Actions for CI/CD  

---

## ðŸ“œ License

This project is licensed under the **MIT License**.

---

## ðŸ™Œ Connect

Built by **Athirath Bommerla**  
â­ Star this repo if you found it helpful!

---

> Let me know if you want to add:
> - âœ… A **live demo badge**
> - ðŸ“¹ A **GIF of the Streamlit dashboard**
> - âš™ï¸ **Auto-deploy instructions**
> 
> and Iâ€™ll help you plug that in too!
