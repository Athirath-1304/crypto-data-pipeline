# Crypto Data Pipeline üöÄ

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Airflow](https://img.shields.io/badge/Airflow-Orchestrated-green)
![License](https://img.shields.io/badge/License-MIT-green)
![Last Commit](https://img.shields.io/github/last-commit/Athirath-1304/crypto-data-pipeline)

---

A production-ready data engineering pipeline that ingests live cryptocurrency data from the [CoinGecko API](https://www.coingecko.com/), validates its schema, saves it as Parquet files, and uploads it to an AWS S3 data lake. The entire workflow is orchestrated using Apache Airflow and modularized for maintainability.

---

## üìå Key Features

- ‚õΩ **Ingestion:** Live data from CoinGecko API (top 100 coins by market cap)
- ‚úÖ **Schema Validation:** Ensures consistent, typed data using PyArrow
- üìÄ **Storage:** Saves daily snapshots locally as `.parquet` files
- ‚òÅÔ∏è **Cloud Upload:** Auto-uploads to AWS S3 as a data lake-ready format
- ‚öôÔ∏è **Orchestration:** End-to-end flow automated via Apache Airflow
- üîÄ **Modular Design:** Separate modules for extract, load, validate
- üîê **Configurable:** Uses `.env` for environment-specific variables

---

## üõ†Ô∏è Tech Stack

- **Python 3.10+**
- **Apache Airflow** (Local setup)
- `pandas`, `requests`, `boto3`, `pyarrow`
- `python-dotenv` for config
- **AWS S3** for cloud storage

---

## üß± Architecture

This project follows a modular pattern and uses Airflow for daily orchestration:

![Crypto Data Pipeline Architecture](assets/pipeline_architecture.png)

---

## üìÇ Folder Structure

\```bash
crypto-data-pipeline/
‚îú‚îÄ‚îÄ crypto_data_pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ config/               # Configuration (future use)
‚îÇ   ‚îú‚îÄ‚îÄ dags/                 # Airflow DAGs
‚îÇ   ‚îú‚îÄ‚îÄ data/                 # Local .parquet data files
‚îÇ   ‚îú‚îÄ‚îÄ extract/              # Fetching data from API
‚îÇ   ‚îú‚îÄ‚îÄ load/                 # Uploading to AWS S3
‚îÇ   ‚îú‚îÄ‚îÄ schema/               # Schema validation logic
‚îÇ   ‚îú‚îÄ‚îÄ logs/                 # Pipeline logging (optional)
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/            # Dev or EDA notebooks
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Entry point (optional if DAG handles all)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ dags/                    # DAG file if outside package
‚îú‚îÄ‚îÄ requirements.txt         # Dependencies
‚îú‚îÄ‚îÄ .env                     # API keys, AWS credentials
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ setup_project.py         # Setup utility (optional)
\```

---

## üöÄ Getting Started

1. **Clone the repo:**
   \```bash
   git clone https://github.com/Athirath-1304/crypto-data-pipeline.git
   cd crypto-data-pipeline
   \```

2. **Install dependencies:**
   \```bash
   pip install -r requirements.txt
   \```

3. **Set up environment variables:**
   Create a `.env` file:
   \```dotenv
   AWS_ACCESS_KEY_ID=...
   AWS_SECRET_ACCESS_KEY=...
   S3_BUCKET_NAME=athirath-crypto-data-lake
   \```

4. **Run Airflow:**
   \```bash
   airflow standalone
   \```

5. **Trigger the DAG:**
   - Open `localhost:8080`
   - Start `crypto_pipeline_dag`

---
## üñ•Ô∏è Airflow DAG UI

> Example snapshot of the DAG running locally:

![DAG Screenshot](assets/airflow_dag.png)


## üìå Status

üü¢ **Stable** ‚Äî currently running locally with full functionality.

Next Steps:
- [ ] Add unit tests
- [ ] Integrate AWS Athena for querying
- [ ] Add CI/CD GitHub Actions

---

## üìú License

This project is licensed under the [MIT License](LICENSE).

---

## üôå Connect

Built by [Athirath Bommerla](https://www.linkedin.com/in/athirathbommerla)

---

> ‚≠ê Star this repo if you found it helpful!
